# With cropping (Mask-RCNN + centre-fallback)
python run_pipeline.py --input_dir data/raw/images --output_dir data/processed/images --model_path models/segmenter/mask_rcnn_molar.pt

# Or without any crop (deskew + centre-resize only)
python run_pipeline.py --input_dir data/raw/images --output_dir data/processed_nocrop --model_path models/segmenter/mask_rcnn_molar.pt --no_crop


# Crop ON, Rotation ON (default)
python .\run_pipeline.py --input_dir .\data\raw\images --output_dir .\data\processed\images --model_path .\models\segmenter\mask_rcnn_molar.pt
# Crop OFF, Rotation ON
python .\run_pipeline.py --input_dir .\data\raw\images --output_dir .\data\processed_nocrop --model_path .\models\segmenter\mask_rcnn_molar.pt --no_crop
# Crop ON, Rotation OFF
python .\run_pipeline.py --input_dir .\data\raw\images --output_dir .\data\processed_norotate --model_path .\models\segmenter\mask_rcnn_molar.pt --no_rotate
# Crop OFF, Rotation OFF
python .\run_pipeline.py --input_dir .\data\raw\images --output_dir .\data\processed_nocrop_norotate --model_path .\models\segmenter\mask_rcnn_molar.pt --no_crop --no_rotate


# Data Augmentation

## one-line command (recommended random legacy augmentation)
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --aug-preset legacy --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

## Exact 10 methods, grouped validation, with motion blur:
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --aug-preset ten --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

## Same but disabling motion blur (method #9 uses a light Gaussian blur substitute):
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --aug-preset ten --no-blur --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

## legacy (strong, albumentations – recommended if installed)
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --aug-preset legacy --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

## simple (lighter, only PIL transforms: flips, small rotations, blur/sharpness, jitter)
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --aug-preset simple --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

## none (no new transformations, just copy originals + group-safe split creation)
python run_augment_records.py --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 0 --make-val --val-frac 0.12 --aug-preset none --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx

#### Other Options
Use --aug-preset ten to generate exactly 10 children per original (1 per method, in a fixed order).
--no-blur disables Motion Blur (method 9) and substitutes a very light Gaussian blur so you still get 10

### After Updates Groupd Val
python -m src.preprocessing.augment_records --input-table data/excel/data_dl.xlsx --images-src data/processed/images --images-dst data/augmented --num-aug-per-image 10 --make-val --val-frac 0.12 --out-csv data/excel/data_dl_augmented.csv --out-xlsx data/excel/data_dl_augmented.xlsx


## 10 Methods of Data Augmentation were:
1. Horizontal flip
2. Vertical flip (low probability)
3. Translation (shift) via Affine
4. Scaling (zoom in/out) via Affine
5. Rotation (± up to ~25°) via Affine
6. Random brightness & contrast adjustment
7. Hue / saturation / value (color) jitter
8. Gaussian noise (light)
9. Motion blur (optional; off if you pass --no_blur)
10. Elastic deformation (mild shape warp)

Note: depending on strength, it may also apply Grid Distortion, Optical Distortion, and Cutout (small occlusions), and it always finishes with a Resize to 512×512 for consistency.

## Aygmentation with Records
python .\run_augment_records.py --input_dir .\data\processed\images --output_dir .\data\augmented --excel_in .\data\excel\data_dl.xlsx --excel_out .\data\excel\data_dl_augmented.xlsx --csv_out .\data\excel\data_dl_augmented.csv --multiplier 10 --size 512 --strength medium --no_blur

# train XGBoost (uses data\excel\data_processed.csv and respects the 'split' column if present)
python models/xgboost_model.py

# train LightGBM
python models/lightgbm_model.py

# XGBoost (evaluates models/outputs/xgb_classifier_pipeline.joblib)
python tests/evaluate_models.py --model xgb

# LightGBM (evaluates models/outputs/lgbm_regressor_pipeline.joblib)
python tests/evaluate_models.py --model lgbm

# (optional) evaluate both if both files exist
python tests/evaluate_models.py --model both

# optional knobs
python tests/evaluate_models.py --model xgb --threshold 0.5 --data data/excel/data_processed.csv --outdir models/outputs




# Platt calibration + threshold tuned for balanced accuracy (default)
python models/xgboost_model.py

# Prefer higher precision? Tune with F1 and slightly harder weights
python models/xgboost_model.py --calibration sigmoid --tune-metric f1     // best results

# If monotone constraints hurt, disable them
python models/xgboost_model.py --no-monotone


# Train calibrated, monotone-constrained LGBM (good default)
python models/lightgbm_model.py

# Softer weights, no monotone, no calibration (for comparison)
python models/lightgbm_model.py --consensus-power 0.5 --no-monotone --calibration none   // best results


# (After retraining XGB + LGBM with the updated files)
python tests/evaluate_models.py --model blend --blend-alpha 0.5

# or bias toward the better single model:
python tests/evaluate_models.py --model blend --blend-alpha 0.6   // If XGB is stronger on positives

# or bias toward the better single model:
python tests/evaluate_models.py --model blend --blend-alpha 0.4 // if LGBM is stronger overall


# 1/ Tune ONLY the blend threshold on the train split (keeping alpha=0.5)
python tests/evaluate_models.py --model blend --tune threshold --blend-alpha 0.5

# 2/ Tune alpha AND threshold jointly on the train split (recommended)
python tests/evaluate_models.py --model blend --tune both --tune-metric f1

# 3/ Fix alpha but override threshold manually
python tests/evaluate_models.py --model blend --blend-alpha 0.6 --threshold 0.3 --tune none






## Note:
#In a clinical workflow, missing an “indirect”
(i.e., saying direct when indirect is needed) is usually the more costly error, so higher recall is desirable.




#Updates with remove noises / min-weight = 0.15

python models/xgboost_model.py --tune-metric accuracy --min-weight 0.15 --consensus-power 0.6 --no-monotone
python models/xgboost_model.py --tune-metric accuracy --min-weight 0.15 --consensus-power 0.6 --no-monotone


# Stack Blend
python models/stack_blend.py --tune-metric accuracy --min-weight 0.15 --consensus-power 0.6


##### Deep Learning Models ####

## Hard labels (Direct vs Indirect):
python run_train_images.py --task hard --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name tf_efficientnet_b3_ns --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42

python models/vision/train_hard.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name tf_efficientnet_b3_ns --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42 --group-col origin_id

## Enable grouped split, TTA, and threshold tuning:
python models/vision/train_hard.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name tf_efficientnet_b3_ns --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42 --group-col origin_id --tta --tune-threshold

## Soft labels (probability of Indirect):
python run_train_images.py --task soft --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name convnext_tiny --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42

python models/vision/train_soft.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name convnext_tiny --img-size 512 --epochs 20 --batch-size 12

## Soft Labels, Enable grouped split, TTA, and threshold tuning:
python models/vision/train_soft.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name tf_efficientnet_b3_ns --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42 --group-col origin_id --tta


## Train both (chain on one line):
python run_train_images.py --task hard --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name tf_efficientnet_b3_ns --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42 && python run_train_images.py --task soft --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --model-name convnext_tiny --img-size 512 --epochs 20 --batch-size 12 --num-workers 4 --seed 42


#### Evaluations

## Hard model only:
python models/vision/eval_models.py --which hard --hard-ckpt weights/vision_hard_best.pt --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --batch-size 32 --out-csv outputs/hard_preds.csv

## Soft model only:
python models/vision/eval_models.py --which soft --soft-ckpt weights/vision_soft_best.pt --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --batch-size 32 --out-csv outputs/soft_preds.csv

## Evaluate both:
python models/vision/eval_models.py --which both --hard-ckpt weights/vision_hard_best.pt --soft-ckpt weights/vision_soft_best.pt --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --batch-size 32 --out-csv outputs/both_preds.csv





################                      #################
################  Hybrid Fusion Model #################

1. Train Base Models
weight/vision_hard_best.pt
weight/vision_soft_best.pt
models/outputs/  # xgb & lgbm artifacts (.pkl/.txt/.json)
data_processed.csv  # has split, y_majority, p_indirect, image_name, features


2. Fit the fusion (calibrate + blend/stack + threshold tune):

python -c "from src.fusion.fuse_train import fit_fusion; fit_fusion(data_csv='data/excel/data_processed.csv', image_root='data/processed/images', weight_dir='weight', ml_dir='models/outputs', out_dir='weight/fusion', calibrator_kind='isotonic', threshold_metric='f1')"

# (If you prefer CLI flags, add argparse to fuse_train.py; the current function can be imported and called from run_pipeline.py.)

3. Peek at the fusion result
# in powershell:
Get-Content .\weight\fusion\fusion_summary.json

4. Single-case inference (clear Direct/Indirect)

python - << 'PY'
import pandas as pd
from src.fusion.fuse_infer import infer_case

df = pd.read_csv("data/excel/data_processed.csv")
row = df[df["split"]=="test"].iloc[0]   # any test row with image_name + features
res = infer_case(
    row,
    image_root="data/processed/images",
    weight_dir="weight",
    ml_dir="models/outputs",
    fusion_dir="weight/fusion"
)
print(res)   # shows p_indirect, threshold, decision, and stream_weights
PY

5. Batch inference over the whole test split → CSV
python - << 'PY'
import pandas as pd
from src.fusion.fuse_infer import infer_case

df = pd.read_csv("data/excel/data_processed.csv")
test = df[df["split"]=="test"].copy()

outs = [infer_case(
            r,
            image_root="data/processed/images",
            weight_dir="weight",
            ml_dir="models/outputs",
            fusion_dir="weight/fusion"
        ) for _, r in test.iterrows()]

test["p_indirect"] = [o["p_indirect"] for o in outs]
test["decision"]   = [o["decision"] for o in outs]
test.to_csv("models/outputs/hybrid_test_predictions.csv", index=False)
print("Saved -> models/outputs/hybrid_test_predictions.csv")
PY


################                                         #################
################  Hybrid Fusion Model (From main fusion) #################

1. Fit the hybrid:
python run_fusion.py train

# Note - Change knobs if you want:
--calibrator platt
--metric youden

# With tabular:
python run_fusion.py train --xgb-model models/outputs/xgb_classifier_pipeline.joblib --lgbm-model models/outputs/lgbm_regressor_pipeline.joblib

2. Inspect the result (threshold, metrics, chosen fusion)
python run_fusion.py info

3. Infer a single case
By index within the test split (default index 0):
python run_fusion.py infer-one

By specific image name:
python run_fusion.py infer-one --image-name 133.jpg

Save to a JSON file and override threshold:
python run_fusion.py infer-one --row-idx 2 --threshold 0.30 --out models/outputs/case_2.json

4. Batch inference → CSV
Over the test split:
python run_fusion.py infer-batch

Over all rows:
python run_fusion.py infer-batch --split all

Over a custom CSV (e.g., external cohort):
python run_fusion.py infer-batch --csv-in path/to/new_cases.csv --out models/outputs/hybrid_new_cases.csv

5. Custom paths (if your structure differs):
python run_fusion.py train --data-csv data/excel/data_processed.csv --image-root data/processed/images --weight-dir weight --ml-dir models/outputs --fusion-dir weight/fusion



###################                                                #########################
################### Deep Learning Enhacements (tf_efficientnet_b4) #########################


1. Progressive training (B4, 384→512) for 3 seeds:
python experiments/vision_v2/train_hard_v2.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --seeds 42,1337,2025 --save-dir weights/v2 --run-name hard_b4_prog

### Test the checkpoints for test cases after training:
python experiments/vision_v2/ensemble_hard.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --ckpts "weights/v2/hard_b4_prog_seed*_stage2_512.pt" --tta

### to test single checkpoint:
python experiments/vision_v2/eval_hard_ckpt.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --ckpt weights/v2/hard_b4_prog_seed2025_stage2_512.pt --tta

2. Ensemble across the 3 seeds (stage‑2 ckpts), with TTA & threshold tuning:
python experiments/vision_v2/ensemble_hard.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --ckpts "weights/v2/hard_b4_prog_seed*_stage2_512.pt" --tta

3. Export prediction CSVs for stacking
- Vision‑hard (ensemble):
python experiments/vision_v2/predict_hard.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --ckpts "weights/v2/hard_b4_prog_seed*_stage2_512.pt" --tta --out-csv out/preds/vis_hard_preds.csv

-Vision‑soft (your best soft ckpt):
python experiments/vision_v2/predict_soft.py --csv-path data/excel/data_dl_augmented.csv --images-root data/augmented --ckpts weights/vision_soft_best.pt --tta --out-csv out/preds/vis_soft_preds.csv

- Tabular (numeric) predictions:
Use your existing XGBoost (hard) and LightGBM (soft) scripts to produce:
 * out/preds/tabular_hard_preds.csv with columns: image_name,origin_id,split,prob_num_hard
 * out/preds/tabular_soft_preds.csv with columns: image_name,origin_id,split,prob_num_soft

4. Train & evaluate the hybrid stacker:
python experiments/vision_v2/stacking/train_stacker.py --vis-hard out/preds/vis_hard_preds.csv --vis-soft out/preds/vis_soft_preds.csv --tab-hard out/preds/tabular_hard_preds.csv --tab-soft out/preds/tabular_soft_preds.csv --save-model weights/v2/stacker_logreg.pkl --report-json out/preds/stacker_report.json




###################                               ##################
###################  Generate Balanced Excel Data ##################

python experiments/data_v2/make_balanced_splits.py --raw-xlsx data/excel/data.xlsx --processed-xlsx data/excel/data_processed.xlsx --dl-xlsx data/excel/data_dl.xlsx --dl-aug-xlsx data/excel/data_dl_augmented.xlsx --processed-xlsx-out data/excel/data_processed.xlsx --processed-csv-out data/excel/data_processed.csv --dl-xlsx-out data/excel/data_dl.xlsx --dl-csv-out data/excel/data_dl.csv --dl-aug-xlsx-out data/excel/data_dl_augmented.xlsx --dl-aug-csv-out data/excel/data_dl_augmented.csv --train-frac 0.70 --val-frac 0.15 --test-frac 0.15 --seed 42 --group-col origin_id --label-col y_majority


######### Final Best Implementation ############
python experiments/fusion_v1/stack_blend.py --xlsx_tab data/excel/data_processed.xlsx --oof_mm weights/mm_dualtask_v1/finalized/oof_val.csv --pred_mm weights/mm_dualtask_v1/finalized/pred_test.csv --oof_mil weights/mil_v1/oof_val.csv --pred_mil weights/mil_v1/pred_test.csv --outdir results/stack_v2 --thr-mode max_acc

Threshold mode: max_acc | target: 0.8 | chosen thr: 0.470
=== OOF === {'auc': 0.8935, 'acc': 0.8456, 'prec': 0.8644, 'rec': 0.9053, 'f1': 0.8844}
=== TEST === {'auc': 0.8695, 'acc': 0.8223, 'prec': 0.8192, 'rec': 0.9062, 'f1': 0.8605}


################                #################
################  UI Gradio App #################

python ui/gradio_app/app.py